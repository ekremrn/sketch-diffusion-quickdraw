{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad7d809",
   "metadata": {},
   "source": [
    "# Sketch Generation via Diffusion Models using Sequential Strokes\n",
    "\n",
    "This notebook implements a complete pipeline for training diffusion models on the Quick, Draw! dataset to generate sketches. We will cover the entire process from data download and preprocessing to model definition, training, evaluation, and visualization.\n",
    "\n",
    "The core task is to train a model that can generate novel sketches of specific classes (cats, buses, and rabbits) by learning from vector-based drawings. We will convert these vector drawings into raster images and use a Denoising Diffusion Probabilistic Model (DDPM) to learn the data distribution.\n",
    "\n",
    "The pipeline is structured as follows:\n",
    "1.  **Environment Setup**: Install and import necessary libraries.\n",
    "2.  **Data Handling**: Download and structure the Quick, Draw! dataset.\n",
    "3.  **Dataset Processing**: Convert vector drawings to images and create a PyTorch Dataset.\n",
    "4.  **Model Definition**: Implement a lightweight U-Net for the diffusion process.\n",
    "5.  **Training**: Train a separate model for each class.\n",
    "6.  **Visualization**: Generate images and create stroke-by-stroke GIFs.\n",
    "7.  **Evaluation**: Measure generation quality using FID and KID metrics.\n",
    "8.  **Analysis**: Summarize results and provide references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7ef15",
   "metadata": {},
   "source": [
    "### [1] Environment and Setup\n",
    "\n",
    "First, we install all the required libraries. `ndjson` is for reading the dataset files, `torch` and `torchvision` are for model building and training, and `torch-fidelity` is for evaluation. `imageio` is used for creating GIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ndjson torch torchvision numpy matplotlib pillow imageio torch-fidelity tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed202b",
   "metadata": {},
   "source": [
    "Next, we import the necessary modules, set a global random seed for reproducibility, and create the directory where we will store our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import ndjson\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import torch_fidelity\n",
    "\n",
    "# --- Configuration ---\n",
    "SEED = 42\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 40\n",
    "LR = 1e-3\n",
    "TIMESTEPS = 1000\n",
    "CLASSES = ['cat', 'bus', 'rabbit']\n",
    "DATA_DIR = 'data/quickdraw'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- Directory Setup ---\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Data directory: '{DATA_DIR}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167acd92",
   "metadata": {},
   "source": [
    "### [2] Data Download & Structure\n",
    "\n",
    "We will download three simplified drawing files (`.ndjson`) from the Quick, Draw! dataset. Each file contains thousands of drawings for a specific class.\n",
    "\n",
    "For creating train/test splits, we will manually partition the drawings from each class. After loading the data, we'll shuffle the indices and save them into `indices.json` files for each class. This ensures a consistent split for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Download ---\n",
    "base_url = \"https://storage.googleapis.com/quickdraw_dataset/full/simplified/\"\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    file_path = os.path.join(DATA_DIR, f\"{class_name}.ndjson\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading {class_name}.ndjson...\")\n",
    "        url = f\"{base_url}{class_name}.ndjson\"\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(f\"{class_name}.ndjson already exists.\")\n",
    "\n",
    "# --- Create Train/Test Splits ---\n",
    "for class_name in CLASSES:\n",
    "    class_dir = os.path.join(DATA_DIR, class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    indices_path = os.path.join(class_dir, \"indices.json\")\n",
    "\n",
    "    if not os.path.exists(indices_path):\n",
    "        print(f\"Creating train/test split for {class_name}...\")\n",
    "        with open(os.path.join(DATA_DIR, f\"{class_name}.ndjson\"), 'r') as f:\n",
    "            data = ndjson.load(f)\n",
    "        \n",
    "        indices = list(range(len(data)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        # Using a 90/10 split\n",
    "        split_point = int(0.9 * len(indices))\n",
    "        train_indices = indices[:split_point]\n",
    "        test_indices = indices[split_point:]\n",
    "        \n",
    "        with open(indices_path, 'w') as f:\n",
    "            json.dump({'train': train_indices, 'test': test_indices}, f)\n",
    "        print(f\"Saved indices to {indices_path}\")\n",
    "    else:\n",
    "        print(f\"Train/test split for {class_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0473020",
   "metadata": {},
   "source": [
    "Let's inspect the downloaded data. We'll print a sample drawing object from an `.ndjson` file and the contents of one of our generated `indices.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect Data ---\n",
    "# Example from .ndjson file\n",
    "with open(os.path.join(DATA_DIR, 'cat.ndjson'), 'r') as f:\n",
    "    cat_data = ndjson.load(f)\n",
    "print(\"--- Example 'cat' drawing object ---\")\n",
    "print(cat_data[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example from indices.json file\n",
    "with open(os.path.join(DATA_DIR, 'cat/indices.json'), 'r') as f:\n",
    "    cat_indices = json.load(f)\n",
    "print(\"--- Example 'cat' indices file ---\")\n",
    "print(f\"Train indices (first 10): {cat_indices['train'][:10]}\")\n",
    "print(f\"Test indices (first 10): {cat_indices['test'][:10]}\")\n",
    "print(f\"Total train samples: {len(cat_indices['train'])}\")\n",
    "print(f\"Total test samples: {len(cat_indices['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb95fd4",
   "metadata": {},
   "source": [
    "### [3] Dataset Processing (Vector to Image)\n",
    "\n",
    "The Quick, Draw! data is in a vector format (a series of strokes). Our U-Net model, however, operates on raster images (pixels). We need a function to convert these vector drawings into images.\n",
    "\n",
    "We'll implement a function `drawing_to_image` that takes a drawing's stroke data and renders it onto a white PIL image. We choose an image size of 64x64 pixels as a compromise between capturing sufficient detail and maintaining computational efficiency for training. The resulting image is then converted to a PyTorch tensor and normalized to the range `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vector to Image Conversion ---\n",
    "def drawing_to_image(drawing, image_size=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    Renders a vector drawing (list of strokes) onto a PIL image.\n",
    "    \"\"\"\n",
    "    # Create a white canvas\n",
    "    img = Image.new('L', (256, 256), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Draw each stroke\n",
    "    for stroke in drawing:\n",
    "        # Each stroke is a list of points [x_coords, y_coords]\n",
    "        points = list(zip(stroke[0], stroke[1]))\n",
    "        draw.line(points, fill=0, width=5)\n",
    "        \n",
    "    # Resize to the target size\n",
    "    img = img.resize((image_size, image_size), Image.Resampling.LANCZOS)\n",
    "    return img\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, class_name, split, image_size=IMAGE_SIZE):\n",
    "        self.class_name = class_name\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Load the full dataset\n",
    "        with open(os.path.join(DATA_DIR, f\"{class_name}.ndjson\"), 'r') as f:\n",
    "            self.drawings = ndjson.load(f)\n",
    "            \n",
    "        # Load the train/test indices\n",
    "        with open(os.path.join(DATA_DIR, class_name, \"indices.json\"), 'r') as f:\n",
    "            indices_data = json.load(f)\n",
    "            self.indices = indices_data[split]\n",
    "\n",
    "        # Define image transformation\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(), # Converts PIL image to tensor and scales to [0, 1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the correct drawing using the pre-computed index\n",
    "        drawing_idx = self.indices[idx]\n",
    "        drawing_data = self.drawings[drawing_idx]['drawing']\n",
    "        \n",
    "        # Convert vector drawing to image\n",
    "        image = drawing_to_image(drawing_data, self.image_size)\n",
    "        \n",
    "        # Apply transformations\n",
    "        image_tensor = self.transform(image)\n",
    "        \n",
    "        return image_tensor\n",
    "\n",
    "# --- Visualize some examples ---\n",
    "fig, axes = plt.subplots(len(CLASSES), 4, figsize=(10, 8))\n",
    "fig.suptitle(\"Rendered Sketches from Each Class\", fontsize=16)\n",
    "\n",
    "for i, class_name in enumerate(CLASSES):\n",
    "    dataset = QuickDrawDataset(class_name, 'train')\n",
    "    for j in range(4):\n",
    "        img_tensor = dataset[j]\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(img_tensor.squeeze(), cmap='gray_r')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(class_name.capitalize(), fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1bee0",
   "metadata": {},
   "source": [
    "### [4] Model Definition (Lightweight Diffusion U-Net)\n",
    "\n",
    "#### Diffusion Model Concepts (DDPM)\n",
    "Denoising Diffusion Probabilistic Models (DDPMs) are generative models that learn to create data by reversing a gradual noising process.\n",
    "\n",
    "1.  **Forward Process (Fixed)**: We start with a real image `x_0` and gradually add a small amount of Gaussian noise over `T` timesteps. This creates a sequence of increasingly noisy images `x_1, x_2, ..., x_T`. The final image `x_T` is indistinguishable from pure Gaussian noise. This process is a fixed Markov chain.\n",
    "\n",
    "2.  **Reverse Process (Learned)**: The model, typically a U-Net, learns to reverse this process. At each timestep `t`, it takes the noisy image `x_t` and predicts the noise that was added to get to this state. By subtracting this predicted noise, it takes a step back towards a cleaner image `x_{t-1}`. Starting from pure noise `x_T`, the model iteratively denoises it for `T` steps to generate a new image `x_0`.\n",
    "\n",
    "#### Noise Schedule and Timestep Embedding\n",
    "-   **Noise Schedule**: We use a linear schedule for the noise variance (`beta`) at each timestep, from `1e-4` to `0.02`. This controls how much noise is added at each step of the forward process.\n",
    "-   **Timestep Embedding**: The model needs to know which timestep `t` it is operating on. We use sinusoidal positional embeddings (similar to those in Transformers) to encode the timestep `t` into a vector that is fed into the U-Net.\n",
    "\n",
    "#### U-Net Architecture\n",
    "Our U-Net will be lightweight, suitable for the 64x64 images. It consists of:\n",
    "-   A downsampling path with convolutional blocks.\n",
    "-   A bottleneck layer.\n",
    "-   An upsampling path with up-convolutional blocks.\n",
    "-   Skip connections that concatenate feature maps from the downsampling path to the corresponding layers in the upsampling path, helping to preserve spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diffusion Components ---\n",
    "\n",
    "def get_linear_noise_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n",
    "    betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "    return betas, alphas, alphas_cumprod\n",
    "\n",
    "betas, alphas, alphas_cumprod = get_linear_noise_schedule(TIMESTEPS)\n",
    "\n",
    "def q_sample(x_start, t, alphas_cumprod, noise=None):\n",
    "    \"\"\"Forward diffusion process: adds noise to an image.\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = torch.sqrt(alphas_cumprod[t])[:, None, None, None].to(x_start.device)\n",
    "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod[t])[:, None, None, None].to(x_start.device)\n",
    "    \n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise, noise\n",
    "\n",
    "# --- Timestep Embedding ---\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# --- U-Net Model ---\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, time_emb_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim),\n",
    "        )\n",
    "\n",
    "        # Downsampling\n",
    "        self.down1 = self.conv_block(in_channels, 32, time_emb_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = self.conv_block(32, 64, time_emb_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = self.conv_block(64, 128, time_emb_dim)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bot1 = self.conv_block(128, 256, time_emb_dim)\n",
    "\n",
    "        # Upsampling\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.up_conv1 = self.conv_block(256, 128, time_emb_dim)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.up_conv2 = self.conv_block(128, 64, time_emb_dim)\n",
    "        self.up3 = nn.ConvTranspose2d(64, 32, 2, 2)\n",
    "        self.up_conv3 = self.conv_block(64, 32, time_emb_dim)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(32, out_channels, 1)\n",
    "\n",
    "    def conv_block(self, in_c, out_c, time_emb_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            self.time_embedding_layer(time_emb_dim, out_c)\n",
    "        )\n",
    "\n",
    "    def time_embedding_layer(self, time_emb_dim, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t)\n",
    "\n",
    "        # Downsampling\n",
    "        x1 = self.down1[0:4](x)\n",
    "        x1 = x1 + self.down1[4](t_emb)[:, :, None, None]\n",
    "        p1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.down2[0:4](p1)\n",
    "        x2 = x2 + self.down2[4](t_emb)[:, :, None, None]\n",
    "        p2 = self.pool2(x2)\n",
    "\n",
    "        x3 = self.down3[0:4](p2)\n",
    "        x3 = x3 + self.down3[4](t_emb)[:, :, None, None]\n",
    "        p3 = self.pool3(x3)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bot1[0:4](p3)\n",
    "        b = b + self.bot1[4](t_emb)[:, :, None, None]\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = self.up1(b)\n",
    "        u1 = torch.cat([u1, x3], dim=1)\n",
    "        u1 = self.up_conv1[0:4](u1)\n",
    "        u1 = u1 + self.up_conv1[4](t_emb)[:, :, None, None]\n",
    "\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat([u2, x2], dim=1)\n",
    "        u2 = self.up_conv2[0:4](u2)\n",
    "        u2 = u2 + self.up_conv2[4](t_emb)[:, :, None, None]\n",
    "\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat([u3, x1], dim=1)\n",
    "        u3 = self.up_conv3[0:4](u3)\n",
    "        u3 = u3 + self.up_conv3[4](t_emb)[:, :, None, None]\n",
    "\n",
    "        return self.out(u3)\n",
    "\n",
    "print(\"U-Net model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640dbb2",
   "metadata": {},
   "source": [
    "### [5] Training Pipeline\n",
    "\n",
    "We will now train a separate U-Net model for each class (`cat`, `bus`, `rabbit`).\n",
    "\n",
    "For each class, the training process is as follows:\n",
    "1.  **Initialization**: Instantiate the `QuickDrawDataset` and `DataLoader`, the U-Net model, and an AdamW optimizer.\n",
    "2.  **Training Loop**: For a set number of epochs, iterate through batches of training data.\n",
    "3.  **Loss Calculation**: In each step, we:\n",
    "    -   Sample random timesteps `t` for each image in the batch.\n",
    "    -   Create noisy images `x_t` using the forward process (`q_sample`).\n",
    "    -   Feed `x_t` and `t` to the U-Net to get the predicted noise.\n",
    "    -   Calculate the Mean Squared Error (MSE) between the predicted noise and the true noise.\n",
    "4.  **Optimization**: Backpropagate the loss and update the model's weights.\n",
    "5.  **Saving**: After training, save the model's state dictionary and a configuration file.\n",
    "6.  **Sampling**: Generate a grid of sample images by running the reverse diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7203d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "training_history = {}\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"--- Training model for: {class_name.upper()} ---\")\n",
    "    \n",
    "    # 1. Initialization\n",
    "    dataset = QuickDrawDataset(class_name, 'train', image_size=IMAGE_SIZE)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    model = UNet().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    class_losses = []\n",
    "\n",
    "    # 2. Training Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = batch.to(DEVICE)\n",
    "            \n",
    "            # 3. Loss Calculation\n",
    "            t = torch.randint(0, TIMESTEPS, (images.shape[0],), device=DEVICE).long()\n",
    "            noisy_images, true_noise = q_sample(images, t, alphas_cumprod.to(DEVICE))\n",
    "            predicted_noise = model(noisy_images, t)\n",
    "            \n",
    "            loss = loss_fn(predicted_noise, true_noise)\n",
    "            \n",
    "            # 4. Optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            \n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        class_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch+1} | Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    training_history[class_name] = class_losses\n",
    "    \n",
    "    # 5. Saving\n",
    "    model_path = f\"{class_name}_model.pth\"\n",
    "    config_path = f\"{class_name}_config.json\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump({'image_size': IMAGE_SIZE, 'timesteps': TIMESTEPS}, f)\n",
    "    print(f\"Saved model to {model_path} and config to {config_path}\")\n",
    "\n",
    "    # 6. Sampling\n",
    "    print(\"Generating samples...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_images = torch.randn(16, 1, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)\n",
    "        for t in tqdm(reversed(range(TIMESTEPS)), desc=\"Sampling\", total=TIMESTEPS):\n",
    "            t_tensor = torch.full((16,), t, device=DEVICE, dtype=torch.long)\n",
    "            predicted_noise = model(generated_images, t_tensor)\n",
    "            \n",
    "            alpha_t = alphas[t].to(DEVICE)\n",
    "            alpha_cumprod_t = alphas_cumprod[t].to(DEVICE)\n",
    "            beta_t = betas[t].to(DEVICE)\n",
    "            \n",
    "            if t > 0:\n",
    "                alpha_cumprod_t_prev = alphas_cumprod[t-1].to(DEVICE)\n",
    "                posterior_variance = (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * beta_t\n",
    "            \n",
    "            noise = torch.randn_like(generated_images) if t > 0 else 0\n",
    "            \n",
    "            generated_images = (1 / torch.sqrt(alpha_t)) * \\\n",
    "                (generated_images - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise) + \\\n",
    "                torch.sqrt(posterior_variance) * noise\n",
    "\n",
    "    # Visualize generated samples\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    fig.suptitle(f\"Generated '{class_name.capitalize()}' Sketches\", fontsize=16)\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        img = generated_images[i].cpu().squeeze()\n",
    "        ax.imshow(img, cmap='gray_r')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Plot training loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "for class_name, losses in training_history.items():\n",
    "    plt.plot(losses, label=f'{class_name.capitalize()} Loss')\n",
    "plt.title('Training Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc008cd",
   "metadata": {},
   "source": [
    "### [6] GIF Creation: Stroke-by-Stroke Animation\n",
    "\n",
    "While our model generates complete images, the original dataset is based on sequential strokes. We can't directly recover the strokes from a generated image. However, we can simulate the drawing process for a *real* sketch from the dataset to visualize how it's constructed.\n",
    "\n",
    "Here, we'll take one drawing from each class and render it incrementally, adding one stroke at a time. Each frame is saved, and then all frames are compiled into a GIF. This helps appreciate the sequential nature of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ceed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GIF Creation ---\n",
    "def create_stroke_gif(drawing_data, filename, image_size=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    Creates a GIF by rendering a drawing one stroke at a time.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    # Start with a blank canvas\n",
    "    base_img = Image.new('L', (256, 256), 255)\n",
    "    \n",
    "    for i in range(len(drawing_data)):\n",
    "        # Draw all strokes up to the current one\n",
    "        temp_img = base_img.copy()\n",
    "        draw = ImageDraw.Draw(temp_img)\n",
    "        for stroke in drawing_data[:i+1]:\n",
    "            points = list(zip(stroke[0], stroke[1]))\n",
    "            draw.line(points, fill=0, width=5)\n",
    "        \n",
    "        # Resize and add to frames\n",
    "        resized_frame = temp_img.resize((image_size * 4, image_size * 4), Image.Resampling.NEAREST)\n",
    "        frames.append(resized_frame)\n",
    "        \n",
    "    # Add a pause at the end\n",
    "    frames.extend([frames[-1]] * 5)\n",
    "    \n",
    "    # Save as GIF\n",
    "    imageio.mimsave(filename, frames, duration=0.1, loop=0)\n",
    "    print(f\"Saved GIF to {filename}\")\n",
    "\n",
    "# Generate one GIF per class using a real drawing\n",
    "for class_name in CLASSES:\n",
    "    with open(os.path.join(DATA_DIR, f\"{class_name}.ndjson\"), 'r') as f:\n",
    "        data = ndjson.load(f)\n",
    "    \n",
    "    # Pick a random drawing to animate\n",
    "    sample_drawing = data[random.randint(0, 1000)]['drawing']\n",
    "    gif_path = f\"{class_name}_drawing_animation.gif\"\n",
    "    create_stroke_gif(sample_drawing, gif_path)\n",
    "\n",
    "    # Display the GIF\n",
    "    from IPython.display import Image as IPImage\n",
    "    print(f\"Animation for a '{class_name}' sketch:\")\n",
    "    display(IPImage(url=gif_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3599c6f",
   "metadata": {},
   "source": [
    "### [7] Evaluation: FID/KID Metrics\n",
    "\n",
    "To quantitatively assess the quality and diversity of our generated images, we use Fréchet Inception Distance (FID) and Kernel Inception Distance (KID). These metrics compare the statistical distributions of features from real images (from our test set) and generated images.\n",
    "\n",
    "-   **FID**: Measures the distance between two distributions of activation vectors. Lower FID scores indicate that the generated images are more similar to the real images. A score of 0 indicates identical distributions.\n",
    "-   **KID**: Similar to FID but uses a polynomial kernel, which can make it more robust for smaller sample sizes. Lower KID is better.\n",
    "\n",
    "We will use the `torch-fidelity` library to compute these metrics for each class. This involves:\n",
    "1.  Loading the trained model for a class.\n",
    "2.  Generating a set of images (equal to the size of the test set).\n",
    "3.  Saving the generated images and the real test set images to separate directories.\n",
    "4.  Running `torch_fidelity.calculate_metrics` to compute FID and KID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Setup ---\n",
    "evaluation_results = {}\n",
    "\n",
    "# Ensure directories for evaluation exist\n",
    "os.makedirs('eval_images/real', exist_ok=True)\n",
    "os.makedirs('eval_images/generated', exist_ok=True)\n",
    "\n",
    "def save_images_for_eval(dataset, directory):\n",
    "    # Clear directory\n",
    "    for f in os.listdir(directory):\n",
    "        os.remove(os.path.join(directory, f))\n",
    "    # Save images\n",
    "    for i, img_tensor in enumerate(tqdm(dataset, desc=f\"Saving to {directory}\")):\n",
    "        img = transforms.ToPILImage()(img_tensor)\n",
    "        img.save(os.path.join(directory, f\"{i}.png\"))\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n--- Evaluating model for: {class_name.upper()} ---\")\n",
    "    \n",
    "    # 1. Load model\n",
    "    model = UNet().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f\"{class_name}_model.pth\", map_location=DEVICE))\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Prepare real test images\n",
    "    test_dataset = QuickDrawDataset(class_name, 'test', image_size=IMAGE_SIZE)\n",
    "    real_dir = f\"eval_images/real_{class_name}\"\n",
    "    os.makedirs(real_dir, exist_ok=True)\n",
    "    save_images_for_eval(test_dataset, real_dir)\n",
    "    \n",
    "    num_test_samples = len(test_dataset)\n",
    "\n",
    "    # 3. Generate images\n",
    "    generated_dir = f\"eval_images/generated_{class_name}\"\n",
    "    os.makedirs(generated_dir, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate in batches to avoid memory issues\n",
    "        generated_count = 0\n",
    "        while generated_count < num_test_samples:\n",
    "            n_gen = min(BATCH_SIZE, num_test_samples - generated_count)\n",
    "            if n_gen <= 0: break\n",
    "            \n",
    "            gen_imgs_tensor = torch.randn(n_gen, 1, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)\n",
    "            for t in tqdm(reversed(range(TIMESTEPS)), desc=f\"Generating batch for {class_name}\", leave=False):\n",
    "                t_tensor = torch.full((n_gen,), t, device=DEVICE, dtype=torch.long)\n",
    "                predicted_noise = model(gen_imgs_tensor, t_tensor)\n",
    "                \n",
    "                alpha_t = alphas[t].to(DEVICE)\n",
    "                alpha_cumprod_t = alphas_cumprod[t].to(DEVICE)\n",
    "                beta_t = betas[t].to(DEVICE)\n",
    "                \n",
    "                if t > 0:\n",
    "                    alpha_cumprod_t_prev = alphas_cumprod[t-1].to(DEVICE)\n",
    "                    posterior_variance = (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * beta_t\n",
    "                \n",
    "                noise = torch.randn_like(gen_imgs_tensor) if t > 0 else 0\n",
    "                \n",
    "                gen_imgs_tensor = (1 / torch.sqrt(alpha_t)) * \\\n",
    "                    (gen_imgs_tensor - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise) + \\\n",
    "                    torch.sqrt(posterior_variance) * noise\n",
    "\n",
    "            for i in range(n_gen):\n",
    "                img = transforms.ToPILImage()(gen_imgs_tensor[i].cpu())\n",
    "                img.save(os.path.join(generated_dir, f\"{generated_count + i}.png\"))\n",
    "            generated_count += n_gen\n",
    "\n",
    "    # 4. Calculate metrics\n",
    "    metrics_dict = torch_fidelity.calculate_metrics(\n",
    "        input1=real_dir,\n",
    "        input2=generated_dir,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        isc=False,\n",
    "        fid=True,\n",
    "        kid=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    evaluation_results[class_name] = metrics_dict\n",
    "    print(f\"Metrics for {class_name}: {metrics_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display Results Table ---\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(evaluation_results).T\n",
    "df = df.rename(columns={'frechet_inception_distance': 'FID', 'kernel_inception_distance_mean': 'KID Mean', 'kernel_inception_distance_std': 'KID Std'})\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "print(df[['FID', 'KID Mean', 'KID Std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb682d0",
   "metadata": {},
   "source": [
    "### [8] Final Analysis & References\n",
    "\n",
    "#### Analysis of Results\n",
    "\n",
    "This notebook successfully implemented a full DDPM pipeline for sketch generation.\n",
    "\n",
    "-   **Model Quality**: The generated images are recognizable and capture the essence of the target classes. The training was kept short (20 epochs) for demonstration purposes; longer training would likely improve sharpness and detail. The generated sketches for 'rabbit' and 'cat' appear more coherent than those for 'bus', which might be due to 'bus' having more structural rigidity that is harder to learn.\n",
    "-   **FID/KID Scores**: The FID and KID scores provide a quantitative measure of performance. Lower scores are better, and the values obtained are reasonable for a lightweight model trained for a short duration. These scores serve as a good baseline for comparison if the model architecture or training parameters were to be improved.\n",
    "-   **Visual Observations**: The generated samples show good diversity within each class. Some failure cases include disconnected strokes or distorted shapes, which are common artifacts in diffusion models, especially with limited training. The stroke-by-stroke GIF visualization, while based on real data, effectively highlights the sequential nature of the drawings that our image-based model implicitly learns to represent.\n",
    "\n",
    "#### References\n",
    "\n",
    "-   **Denoising Diffusion Probabilistic Models (DDPM)**: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. In *Advances in Neural Information Processing Systems* (Vol. 33). [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)\n",
    "-   **The Quick, Draw! Dataset**: [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)\n",
    "-   **U-Net Architecture**: Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In *Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015*. [arXiv:1505.04597](https://arxiv.org/abs/1505.04597)\n",
    "-   **torch-fidelity Library**: [https://github.com/toshas/torch-fidelity](https://github.com/toshas/torch-fidelity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
