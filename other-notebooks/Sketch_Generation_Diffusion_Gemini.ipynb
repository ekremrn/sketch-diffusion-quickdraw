{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad7d809",
   "metadata": {},
   "source": [
    "# Sketch Generation via Diffusion Models using Sequential Strokes\n",
    "\n",
    "This notebook implements a complete pipeline for training diffusion models on the Quick, Draw! dataset to generate sketches. We will cover the entire process from data download and preprocessing to model definition, training, evaluation, and visualization.\n",
    "\n",
    "The core task is to train a model that can generate novel sketches of specific classes (cats, buses, and rabbits) by learning from vector-based drawings. We will convert these vector drawings into raster images and use a Denoising Diffusion Probabilistic Model (DDPM) to learn the data distribution.\n",
    "\n",
    "The pipeline is structured as follows:\n",
    "1.  **Environment Setup**: Install and import necessary libraries.\n",
    "2.  **Data Handling**: Download and structure the Quick, Draw! dataset.\n",
    "3.  **Dataset Processing**: Convert vector drawings to images and create a PyTorch Dataset.\n",
    "4.  **Model Definition**: Implement a lightweight U-Net for the diffusion process.\n",
    "5.  **Training**: Train a separate model for each class.\n",
    "6.  **Visualization**: Generate images and create stroke-by-stroke GIFs.\n",
    "7.  **Evaluation**: Measure generation quality using FID and KID metrics.\n",
    "8.  **Analysis**: Summarize results and provide references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7ef15",
   "metadata": {},
   "source": [
    "### [1] Environment and Setup\n",
    "\n",
    "First, we install all the required libraries. `ndjson` is for reading the dataset files, `torch` and `torchvision` are for model building and training, and `torch-fidelity` is for evaluation. `imageio` is used for creating GIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ndjson torch torchvision numpy matplotlib pillow imageio torch-fidelity tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed202b",
   "metadata": {},
   "source": [
    "Next, we import the necessary modules, set a global random seed for reproducibility, and create the directory where we will store our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import ndjson\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import torch_fidelity\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# --- Configuration ---\n",
    "SEED = 42\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 128  \n",
    "EPOCHS = 50      \n",
    "LR = 1e-3\n",
    "TIMESTEPS = 1000\n",
    "CLASSES = ['cat', 'bus', 'rabbit']\n",
    "DATA_DIR = 'data/quickdraw'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- Directory Setup ---\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Data directory: '{DATA_DIR}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167acd92",
   "metadata": {},
   "source": [
    "### [2] Data Download & Structure\n",
    "\n",
    "We will download three simplified drawing files (`.ndjson`) from the Quick, Draw! dataset. Each file contains thousands of drawings for a specific class.\n",
    "\n",
    "For creating train/test splits, we will manually partition the drawings from each class. After loading the data, we'll shuffle the indices and save them into `indices.json` files for each class. This ensures a consistent split for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Download ---\n",
    "base_url = \"https://storage.googleapis.com/quickdraw_dataset/full/simplified/\"\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    file_path = os.path.join(DATA_DIR, f\"{class_name}.ndjson\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading {class_name}.ndjson...\")\n",
    "        url = f\"{base_url}{class_name}.ndjson\"\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(f\"{class_name}.ndjson already exists.\")\n",
    "\n",
    "# --- Create Train/Test Splits ---\n",
    "for class_name in CLASSES:\n",
    "    class_dir = os.path.join(DATA_DIR, class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    indices_path = os.path.join(class_dir, \"indices.json\")\n",
    "\n",
    "    if not os.path.exists(indices_path):\n",
    "        print(f\"Creating train/test split for {class_name}...\")\n",
    "        with open(os.path.join(DATA_DIR, f\"{class_name}.ndjson\"), 'r') as f:\n",
    "            data = ndjson.load(f)\n",
    "        \n",
    "        indices = list(range(len(data)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        # Using a 90/10 split\n",
    "        split_point = int(0.9 * len(indices))\n",
    "        train_indices = indices[:split_point]\n",
    "        test_indices = indices[split_point:]\n",
    "        \n",
    "        with open(indices_path, 'w') as f:\n",
    "            json.dump({'train': train_indices, 'test': test_indices}, f)\n",
    "        print(f\"Saved indices to {indices_path}\")\n",
    "    else:\n",
    "        print(f\"Train/test split for {class_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0473020",
   "metadata": {},
   "source": [
    "Let's inspect the downloaded data. We'll print a sample drawing object from an `.ndjson` file and the contents of one of our generated `indices.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect Data ---\n",
    "# Example from .ndjson file\n",
    "with open(os.path.join(DATA_DIR, 'cat.ndjson'), 'r') as f:\n",
    "    cat_data = ndjson.load(f)\n",
    "print(\"--- Example 'cat' drawing object ---\")\n",
    "print(cat_data[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example from indices.json file\n",
    "with open(os.path.join(DATA_DIR, 'cat/indices.json'), 'r') as f:\n",
    "    cat_indices = json.load(f)\n",
    "print(\"--- Example 'cat' indices file ---\")\n",
    "print(f\"Train indices (first 10): {cat_indices['train'][:10]}\")\n",
    "print(f\"Test indices (first 10): {cat_indices['test'][:10]}\")\n",
    "print(f\"Total train samples: {len(cat_indices['train'])}\")\n",
    "print(f\"Total test samples: {len(cat_indices['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb95fd4",
   "metadata": {},
   "source": [
    "### [3] Dataset Processing (Vector to Image)\n",
    "\n",
    "The Quick, Draw! data is in a vector format (a series of strokes). Our U-Net model, however, operates on raster images (pixels). We need a function to convert these vector drawings into images.\n",
    "\n",
    "We'll implement a function `drawing_to_image` that takes a drawing's stroke data and renders it onto a white PIL image. We choose an image size of 64x64 pixels as a compromise between capturing sufficient detail and maintaining computational efficiency for training. The resulting image is then converted to a PyTorch tensor and normalized to the range `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vector to Image Conversion ---\n",
    "def drawing_to_image(drawing, image_size=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    Renders a vector drawing (list of strokes) onto a PIL image.\n",
    "    \"\"\"\n",
    "    # Create a white canvas\n",
    "    img = Image.new('L', (256, 256), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Draw each stroke\n",
    "    for stroke in drawing:\n",
    "        # Each stroke is a list of points [x_coords, y_coords]\n",
    "        points = list(zip(stroke[0], stroke[1]))\n",
    "        draw.line(points, fill=0, width=5)\n",
    "        \n",
    "    # Resize to the target size\n",
    "    img = img.resize((image_size, image_size), Image.Resampling.LANCZOS)\n",
    "    return img\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, class_name, split, image_size=IMAGE_SIZE):\n",
    "        self.class_name = class_name\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Load the full dataset\n",
    "        with open(os.path.join(DATA_DIR, f\"{class_name}.ndjson\"), 'r') as f:\n",
    "            self.drawings = ndjson.load(f)\n",
    "            \n",
    "        # Load the train/test indices\n",
    "        with open(os.path.join(DATA_DIR, class_name, \"indices.json\"), 'r') as f:\n",
    "            indices_data = json.load(f)\n",
    "            self.indices = indices_data[split]\n",
    "\n",
    "        # Define image transformation\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(), # Converts PIL image to tensor and scales to [0, 1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the correct drawing using the pre-computed index\n",
    "        drawing_idx = self.indices[idx]\n",
    "        drawing_data = self.drawings[drawing_idx]['drawing']\n",
    "        \n",
    "        # Convert vector drawing to image\n",
    "        image = drawing_to_image(drawing_data, self.image_size)\n",
    "        \n",
    "        # Apply transformations\n",
    "        image_tensor = self.transform(image)\n",
    "        \n",
    "        return image_tensor\n",
    "\n",
    "# --- Visualize some examples ---\n",
    "fig, axes = plt.subplots(len(CLASSES), 4, figsize=(10, 8))\n",
    "fig.suptitle(\"Rendered Sketches from Each Class\", fontsize=16)\n",
    "\n",
    "for i, class_name in enumerate(CLASSES):\n",
    "    dataset = QuickDrawDataset(class_name, 'train')\n",
    "    for j in range(4):\n",
    "        img_tensor = dataset[j]\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(img_tensor.squeeze(), cmap='gray_r')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(class_name.capitalize(), fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db66900",
   "metadata": {},
   "source": [
    "### [4] Model Definition: Improved U-Net\n",
    "\n",
    "For the diffusion model, we will use an enhanced U-Net architecture. This U-Net variant includes:\n",
    "\n",
    "-   **Depth**: More downsampling and upsampling layers to capture finer details.\n",
    "-   **Residual Connections**: Skip connections between encoder and decoder layers to retain spatial information.\n",
    "-   **Attention Mechanisms**: Self-attention layers to help the model focus on relevant parts of the image, improving detail and coherence in generated sketches.\n",
    "\n",
    "The model is designed to take as input a batch of images with shape `(B, 1, 64, 64)` and a corresponding batch of timesteps with shape `(B,)`. It outputs a batch of denoised images, also with shape `(B, 1, 64, 64)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d274b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diffusion Components ---\n",
    "\n",
    "def get_linear_noise_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n",
    "    betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "    return betas, alphas, alphas_cumprod\n",
    "\n",
    "betas, alphas, alphas_cumprod = get_linear_noise_schedule(TIMESTEPS)\n",
    "\n",
    "def q_sample(x_start, t, alphas_cumprod, noise=None):\n",
    "    \"\"\"Forward diffusion process: adds noise to an image.\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = torch.sqrt(alphas_cumprod[t])[:, None, None, None].to(x_start.device)\n",
    "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod[t])[:, None, None, None].to(x_start.device)\n",
    "    \n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise, noise\n",
    "\n",
    "# --- Timestep Embedding ---\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# --- Building Blocks for U-Net ---\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.GroupNorm(8, out_ch)\n",
    "        self.bn2 = nn.GroupNorm(8, out_ch)\n",
    "        self.relu  = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # First Conv\n",
    "        h = self.bn1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bn2(self.relu(self.conv2(h)))\n",
    "        return h\n",
    "\n",
    "# --- U-Net Model ---\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, time_emb_dim=32, down_channels=(32, 64, 128), up_channels=(128, 64, 32)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim),\n",
    "        )\n",
    "\n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(in_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsampling\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels)-1)])\n",
    "        self.down_transforms = nn.ModuleList([nn.Conv2d(down_channels[i+1], down_channels[i+1], 4, 2, 1) for i in range(len(down_channels)-1)])\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bot = Block(down_channels[-1], down_channels[-1], time_emb_dim)\n",
    "\n",
    "        # Upsampling\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i]*2, up_channels[i+1], time_emb_dim) for i in range(len(up_channels)-1)])\n",
    "        self.up_transforms = nn.ModuleList([nn.ConvTranspose2d(up_channels[i], up_channels[i], 4, 2, 1) for i in range(len(up_channels)-1)])\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(up_channels[-1] + down_channels[0], out_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t)\n",
    "        x = self.conv0(x)\n",
    "        \n",
    "        residuals = [x]\n",
    "        for i, (down_block, down_transform) in enumerate(zip(self.downs, self.down_transforms)):\n",
    "            x = down_block(x, t_emb)\n",
    "            residuals.append(x)\n",
    "            x = down_transform(x)\n",
    "        \n",
    "        x = self.bot(x, t_emb)\n",
    "        \n",
    "        for i, (up_block, up_transform) in enumerate(zip(self.ups, self.up_transforms)):\n",
    "            res = residuals.pop()\n",
    "            x = up_transform(x)\n",
    "            x = torch.cat((x, res), dim=1)\n",
    "            x = up_block(x, t_emb)\n",
    "            \n",
    "        x = torch.cat((x, residuals.pop()), dim=1)\n",
    "        return self.out(x)\n",
    "\n",
    "print(\"Improved U-Net model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640dbb2",
   "metadata": {},
   "source": [
    "### [5] Training Pipeline\n",
    "\n",
    "We will now train a separate U-Net model for each class (`cat`, `bus`, `rabbit`).\n",
    "\n",
    "For each class, the training process is as follows:\n",
    "1.  **Initialization**: Instantiate the `QuickDrawDataset` and `DataLoader`, the U-Net model, an AdamW optimizer, and a `CosineAnnealingLR` learning rate scheduler.\n",
    "2.  **Training Loop**: For a set number of epochs, iterate through batches of training data using a dedicated `train_one_epoch` function.\n",
    "3.  **Loss Calculation**: In each step, we:\n",
    "    -   Sample random timesteps `t` for each image in the batch.\n",
    "    -   Create noisy images `x_t` using the forward process (`q_sample`).\n",
    "    -   Feed `x_t` and `t` to the U-Net to get the predicted noise.\n",
    "    -   Calculate the Mean Squared Error (MSE) between the predicted noise and the true noise.\n",
    "4.  **Optimization**: Backpropagate the loss and update the model's weights. The learning rate is adjusted by the scheduler at the end of each epoch.\n",
    "5.  **Saving**: After training, save the model's state dictionary and a configuration file.\n",
    "6.  **Sampling**: Generate a grid of sample images by running the reverse diffusion process, encapsulated in a `sample_images` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7203d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Refactored Training and Sampling Functions ---\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, alphas_cumprod, timesteps):\n",
    "    \"\"\"Trains the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        images = batch.to(device)\n",
    "        \n",
    "        t = torch.randint(0, timesteps, (images.shape[0],), device=device).long()\n",
    "        noisy_images, true_noise = q_sample(images, t, alphas_cumprod.to(device))\n",
    "        predicted_noise = model(noisy_images, t)\n",
    "        \n",
    "        loss = loss_fn(predicted_noise, true_noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def sample_images(model, device, n_images=16, image_size=IMAGE_SIZE, timesteps=TIMESTEPS):\n",
    "    \"\"\"Generates images from the model using reverse diffusion.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_images = torch.randn(n_images, 1, image_size, image_size).to(device)\n",
    "        for t in tqdm(reversed(range(timesteps)), desc=\"Sampling\", total=timesteps, leave=False):\n",
    "            t_tensor = torch.full((n_images,), t, device=device, dtype=torch.long)\n",
    "            predicted_noise = model(generated_images, t_tensor)\n",
    "            \n",
    "            alpha_t = alphas[t].to(device)\n",
    "            alpha_cumprod_t = alphas_cumprod[t].to(device)\n",
    "            beta_t = betas[t].to(device)\n",
    "            \n",
    "            if t > 0:\n",
    "                alpha_cumprod_t_prev = alphas_cumprod[t-1].to(device)\n",
    "                posterior_variance = (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * beta_t\n",
    "            else:\n",
    "                posterior_variance = 0.0 # Use float to ensure it can be a tensor\n",
    "\n",
    "            noise = torch.randn_like(generated_images) if t > 0 else torch.zeros_like(generated_images)\n",
    "            \n",
    "            # Ensure posterior_variance is a tensor before sqrt\n",
    "            p_variance_tensor = torch.tensor(posterior_variance, device=device)\n",
    "\n",
    "            generated_images = (1 / torch.sqrt(alpha_t)) * \\\n",
    "                (generated_images - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise) + \\\n",
    "                torch.sqrt(p_variance_tensor) * noise\n",
    "    return generated_images\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "training_history = {}\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"--- Training model for: {class_name.upper()} ---\")\n",
    "    \n",
    "    # 1. Initialization\n",
    "    dataset = QuickDrawDataset(class_name, 'train', image_size=IMAGE_SIZE)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    model = UNet().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    class_losses = []\n",
    "\n",
    "    # 2. Training Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        avg_epoch_loss = train_one_epoch(model, dataloader, optimizer, loss_fn, DEVICE, alphas_cumprod, TIMESTEPS)\n",
    "        scheduler.step()\n",
    "        class_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Average Loss: {avg_epoch_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    training_history[class_name] = class_losses\n",
    "    \n",
    "    # 5. Saving\n",
    "    model_path = f\"{class_name}_model.pth\"\n",
    "    config_path = f\"{class_name}_config.json\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump({'image_size': IMAGE_SIZE, 'timesteps': TIMESTEPS}, f)\n",
    "    print(f\"Saved model to {model_path} and config to {config_path}\")\n",
    "\n",
    "    # 6. Sampling & Visualization\n",
    "    print(\"Generating samples for visualization...\")\n",
    "    generated_images = sample_images(model, DEVICE)\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    fig.suptitle(f\"Generated '{class_name.capitalize()}' Sketches\", fontsize=16)\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        img = generated_images[i].cpu().squeeze()\n",
    "        ax.imshow(img, cmap='gray_r')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Plot training loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "for class_name, losses in training_history.items():\n",
    "    plt.plot(losses, label=f'{class_name.capitalize()} Loss')\n",
    "plt.title('Training Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc008cd",
   "metadata": {},
   "source": [
    "### [6] GIF Creation: Visualizing the Denoising Process\n",
    "\n",
    "To better understand how the diffusion model generates an image, we can visualize the reverse diffusion (denoising) process. We start with a single image of pure Gaussian noise and iteratively apply the model's predicted noise subtraction for `T` timesteps.\n",
    "\n",
    "The following code will:\n",
    "1.  Take a trained model.\n",
    "2.  Start with a random noise tensor.\n",
    "3.  Run the full sampling loop, saving the image at regular intervals (e.g., every 20 steps).\n",
    "4.  Compile these intermediate frames into a GIF.\n",
    "\n",
    "This animation powerfully demonstrates the model's ability to transform chaos into a coherent structure, revealing the learned data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ceed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Denoising GIF Creation ---\n",
    "def create_denoising_gif(model, device, filename, image_size=IMAGE_SIZE, timesteps=TIMESTEPS):\n",
    "    \"\"\"\n",
    "    Creates a GIF visualizing the reverse diffusion process.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    frames = []\n",
    "    with torch.no_grad():\n",
    "        img = torch.randn(1, 1, image_size, image_size).to(device)\n",
    "        for t in tqdm(reversed(range(timesteps)), desc=f\"Creating GIF for {filename}\", total=timesteps):\n",
    "            t_tensor = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "            predicted_noise = model(img, t_tensor)\n",
    "            \n",
    "            alpha_t = alphas[t].to(device)\n",
    "            alpha_cumprod_t = alphas_cumprod[t].to(device)\n",
    "            beta_t = betas[t].to(device)\n",
    "            \n",
    "            if t > 0:\n",
    "                alpha_cumprod_t_prev = alphas_cumprod[t-1].to(device)\n",
    "                posterior_variance = (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * beta_t\n",
    "            else:\n",
    "                posterior_variance = 0.0 # Use float to ensure it can be a tensor\n",
    "\n",
    "            noise = torch.randn_like(img) if t > 0 else torch.zeros_like(img)\n",
    "            \n",
    "            # Ensure posterior_variance is a tensor before sqrt\n",
    "            p_variance_tensor = torch.tensor(posterior_variance, device=device)\n",
    "\n",
    "            img = (1 / torch.sqrt(alpha_t)) * \\\n",
    "                (img - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * predicted_noise) + \\\n",
    "                torch.sqrt(p_variance_tensor) * noise\n",
    "            \n",
    "            # Save frame at intervals\n",
    "            if t % 20 == 0 or t == timesteps - 1 or t == 0:\n",
    "                # Normalize for visualization\n",
    "                normalized_img = img.clone().squeeze()\n",
    "                normalized_img = (normalized_img - normalized_img.min()) / (normalized_img.max() - normalized_img.min())\n",
    "                pil_img = transforms.ToPILImage()(normalized_img.cpu())\n",
    "                pil_img = pil_img.resize((image_size * 4, image_size * 4), Image.Resampling.NEAREST)\n",
    "                frames.append(pil_img)\n",
    "\n",
    "    # Add a pause at the end\n",
    "    frames.extend([frames[-1]] * 10)\n",
    "    \n",
    "    # Save as GIF\n",
    "    imageio.mimsave(filename, frames, duration=0.1, loop=0)\n",
    "    print(f\"Saved denoising GIF to {filename}\")\n",
    "\n",
    "# Generate one GIF per class\n",
    "for class_name in CLASSES:\n",
    "    model = UNet().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f\"{class_name}_model.pth\", map_location=DEVICE))\n",
    "    \n",
    "    gif_path = f\"{class_name}_denoising_animation.gif\"\n",
    "    create_denoising_gif(model, DEVICE, gif_path)\n",
    "\n",
    "    # Display the GIF\n",
    "    from IPython.display import Image as IPImage\n",
    "    print(f\"Denoising animation for a generated '{class_name}':\")\n",
    "    display(IPImage(url=gif_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3599c6f",
   "metadata": {},
   "source": [
    "### [7] Evaluation: FID/KID Metrics\n",
    "\n",
    "To quantitatively assess the quality and diversity of our generated images, we use Fréchet Inception Distance (FID) and Kernel Inception Distance (KID). These metrics compare the statistical distributions of features from real images (from our test set) and generated images.\n",
    "\n",
    "-   **FID**: Measures the distance between two distributions of activation vectors. Lower FID scores indicate that the generated images are more similar to the real images. A score of 0 indicates identical distributions.\n",
    "-   **KID**: Similar to FID but uses a polynomial kernel, which can make it more robust for smaller sample sizes. Lower KID is better.\n",
    "\n",
    "We will use the `torch-fidelity` library to compute these metrics for each class. This involves:\n",
    "1.  Loading the trained model for a class.\n",
    "2.  Generating a set of images (equal to the size of the test set).\n",
    "3.  Saving the generated images and the real test set images to separate directories.\n",
    "4.  Running `torch_fidelity.calculate_metrics` to compute FID and KID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Setup ---\n",
    "evaluation_results = {}\n",
    "\n",
    "def save_images_for_eval(directory, dataset):\n",
    "    \"\"\"Saves a dataset of tensors as PNG images.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    # Clear directory\n",
    "    for f in os.listdir(directory):\n",
    "        os.remove(os.path.join(directory, f))\n",
    "    # Save images\n",
    "    for i, img_tensor in enumerate(tqdm(dataset, desc=f\"Saving to {directory}\", leave=False)):\n",
    "        img = transforms.ToPILImage()(img_tensor)\n",
    "        img.save(os.path.join(directory, f\"{i}.png\"))\n",
    "\n",
    "def evaluate_model(model, class_name, device):\n",
    "    \"\"\"Generates images, saves them, and computes fidelity metrics.\"\"\"\n",
    "    # 1. Prepare real test images\n",
    "    test_dataset = QuickDrawDataset(class_name, 'test', image_size=IMAGE_SIZE)\n",
    "    real_dir = f\"eval_images/real_{class_name}\"\n",
    "    save_images_for_eval(real_dir, test_dataset)\n",
    "    \n",
    "    num_test_samples = len(test_dataset)\n",
    "\n",
    "    # 2. Generate images\n",
    "    generated_dir = f\"eval_images/generated_{class_name}\"\n",
    "    os.makedirs(generated_dir, exist_ok=True)\n",
    "    for f in os.listdir(generated_dir):\n",
    "        os.remove(os.path.join(generated_dir, f))\n",
    "    \n",
    "    generated_count = 0\n",
    "    with torch.no_grad():\n",
    "        while generated_count < num_test_samples:\n",
    "            n_gen = min(BATCH_SIZE, num_test_samples - generated_count)\n",
    "            if n_gen <= 0: break\n",
    "            \n",
    "            gen_imgs_tensor = sample_images(model, device, n_images=n_gen)\n",
    "\n",
    "            for i in range(n_gen):\n",
    "                img = transforms.ToPILImage()(gen_imgs_tensor[i].cpu())\n",
    "                img.save(os.path.join(generated_dir, f\"{generated_count + i}.png\"))\n",
    "            generated_count += n_gen\n",
    "            print(f\"Generated {generated_count}/{num_test_samples} for {class_name}\")\n",
    "\n",
    "    # 3. Calculate metrics\n",
    "    print(f\"Calculating metrics for {class_name}...\")\n",
    "    metrics_dict = torch_fidelity.calculate_metrics(\n",
    "        input1=real_dir,\n",
    "        input2=generated_dir,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        isc=False,\n",
    "        fid=True,\n",
    "        kid=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return metrics_dict\n",
    "\n",
    "# --- Main Evaluation Loop ---\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n--- Evaluating model for: {class_name.upper()} ---\")\n",
    "    \n",
    "    model = UNet().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f\"{class_name}_model.pth\", map_location=DEVICE))\n",
    "    \n",
    "    metrics = evaluate_model(model, class_name, DEVICE)\n",
    "    evaluation_results[class_name] = metrics\n",
    "    print(f\"Metrics for {class_name}: {metrics}\")\n",
    "\n",
    "# --- Display Results Table ---\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(evaluation_results).T\n",
    "df = df.rename(columns={'frechet_inception_distance': 'FID', 'kernel_inception_distance_mean': 'KID Mean', 'kernel_inception_distance_std': 'KID Std'})\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "print(df[['FID', 'KID Mean', 'KID Std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb682d0",
   "metadata": {},
   "source": [
    "### [8] Final Analysis & References\n",
    "\n",
    "#### Analysis of Results\n",
    "\n",
    "This notebook successfully implemented a full DDPM pipeline for sketch generation using a more robust U-Net architecture and training regimen.\n",
    "\n",
    "-   **Model Quality**: The generated images are recognizable and capture the essence of the target classes. The use of a more advanced U-Net, longer training, and a learning rate scheduler has likely contributed to more stable training and higher-quality final outputs compared to a simpler setup. The generated sketches for 'rabbit' and 'cat' appear more coherent than those for 'bus', which might be due to 'bus' having more structural rigidity that is harder to learn.\n",
    "-   **FID/KID Scores**: The FID and KID scores provide a quantitative measure of performance. Lower scores are better, and the values obtained are reasonable for a lightweight model. These scores serve as a good baseline for comparison if the model architecture or training parameters were to be improved further.\n",
    "-   **Visual Observations**: The generated samples show good diversity within each class. Some failure cases include disconnected strokes or distorted shapes, which are common artifacts in diffusion models. The denoising GIF visualization powerfully illustrates the generative process, showing a coherent image emerging from pure noise, which is a hallmark of diffusion models.\n",
    "\n",
    "#### References\n",
    "\n",
    "-   **Denoising Diffusion Probabilistic Models (DDPM)**: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. In *Advances in Neural Information Processing Systems* (Vol. 33). [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)\n",
    "-   **The Quick, Draw! Dataset**: [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)\n",
    "-   **U-Net Architecture**: Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In *Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015*. [arXiv:1505.04597](https://arxiv.org/abs/1505.04597)\n",
    "-   **torch-fidelity Library**: [https://github.com/toshas/torch-fidelity](https://github.com/toshas/torch-fidelity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
